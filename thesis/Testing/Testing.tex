\chapter{Modular Testing}

The testing process for this project was conducted in a systematic and incremental manner to ensure that each subsystem functioned as expected before being integrated into the final system. The approach involved testing each module independently, followed by creating interfaces between these modules, and finally testing the interactions between them. This strategy helped identify and resolve issues early, ensuring that the final system was robust and performed as required.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Testing/figs/flow.png}
	\caption{Illustration of different levels of testing implemented in the project.}
	\label{fig:testing_levels}
\end{figure}

\section{Independent Module Testing}

Each subsystem of the robot was tested individually using Python scripts to verify functionality before integration into the full system. This modular testing allowed for focused troubleshooting and ensured that each component operated as expected in isolation.

\subsection{Web Interface Testing}

The first step was testing the \textbf{web interface} to verify that the server could host a webpage and be accessed by devices connected to the same network. This involved the following steps:

\begin{enumerate}
	\item Setting up a simple Flask server.
	\item Connecting multiple devices (e.g., laptop, smartphone) to the same network.
	\item Attempting to access the server from each device.
	\item Verifying the server responded correctly to basic requests.
\end{enumerate}

 A script was developed to send ping requests to the server every 5 seconds over a 2-hour period (1440 pings total). Uptime percentage was calculated as (successful pings / total pings) * 100.  Packet loss was measured using the 'ping' command with 1000 packets sent from devices at 1m intervals from the server. Significant packet loss (\(>5\)\%) was first observed at distances beyond 5 meters.
 

\textbf{Results:}
\begin{itemize}
	\item Server connection success rate: \textbf{100\%} within 2 meters.
	\item Connection stability: \textbf{99.5\% uptime} over 2 hours.
	\item Issues: Packet loss at distances exceeding 5 meters.
\end{itemize}

 The server was accessible from all devices within a \textbf{2-meter range} with minimal latency. Stability was measured at \textbf{99.5\% uptime} over a 2-hour test, with some packet loss beyond \textbf{5 meters}. These foundational tests ensured that the control interface could be hosted and accessed remotely, which was essential for subsequent integration.

\subsection{Motor Control Testing}

The \textbf{motor control} was tested using a basic Python script with the GPIO library to send PWM signals to the motors via the L298N H-bridge. The following steps were taken:

\begin{enumerate}
	\item Connecting each motor pair to the H-bridge.
	\item Writing a script to send PWM signals to control speed and direction.
	\item Placing markers on the wheels for visual tracking.
	\item Recording slow-motion video to check rotations per minute (RPM).
\end{enumerate}

Results showed that the motors rotated consistently at \textbf{90 RPM} at full speed when using 9V source and lifted above the ground, with a slight deviation of \textbf{2 RPM} between motors. Testing revealed that the PWM signal had to be increased to \textbf{80\%} to enable smooth turning, addressing friction and motor load issues.

\textbf{Results:}
\begin{itemize}
	\item Maximum wheel speed: \textbf{80 RPM} at 100\% PWM.
	\item Wheel speed deviation: \textbf{2 RPM}.
	\item Minimum Required PWM for driving forward and backward: \textbf{30\%}
	\item Minimum Required PWM for turning: \textbf{70\%}.
\end{itemize}


\subsection{ArUco Marker Detection Testing}
To evaluate the system, we performed tests focusing on key performance indicators: detection capabilities, distance measurement accuracy, speed estimation, and system responsiveness. Experiments were conducted under controlled conditions with varied lighting and distances to simulate real-world environments. Results were analyzed using Python libraries such as NumPy and Pandas.

\subsubsection{Detection Performance Testing}
Data Collection: 100 images were captured at 0.1m intervals from 0.1m to 2m under three lighting conditions. Detection attempts were logged for each image.

\textbf{Data Collection:} 
A total of 100 images were captured at 0.1m intervals from 0.1m to 2m under three lighting conditions (normal, low light, bright light). Each detection attempt was recorded, and data was gathered for comparison.

\begin{table}[H]
	\centering
	\caption{ArUco Marker Detection Performance}
	\label{tab:aruco_detection}
	\begin{tabular}{lccc}
		\hline
		\textbf{Condition} & \textbf{100\% Detection} & \textbf{$>$90\% Detection} & \textbf{Avg. Detection Time} \\
		\hline
		Normal (500 lux) & Up to 1.5m & Up to 1.8m & \multirow{3}{*}{3.2ms (±0.5ms)} \\
		Low Light (100 lux) & Up to 1.2m & Up to 1.5m & \\
		Bright Light (1000 lux) & Up to 1.7m & Up to 2.0m & \\
		\hline
	\end{tabular}
\end{table}


\begin{figure}[!htb]
	\centering
	% Add horizontal space between figures
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\begin{tikzpicture}[scale=1]  % Slightly scaled down to ensure margin compliance
			\begin{axis}[
				xlabel={Distance (m)},
				ylabel={Detection Rate (\%)},
				xmin=0, xmax=2,
				ymin=80, ymax=100,
				legend pos=south west,
				ymajorgrids=true,
				grid style=dashed,
				width=\textwidth,  % Ensure figure fits within minipage
				]
				
				\addplot[
				color=blue,
				mark=square,
				]
				coordinates {
					(0.5,100)(1.0,100)(1.5,100)(1.8,95)(2.0,90)
				};
				\addlegendentry{Normal Light}
				
				\addplot[
				color=red,
				mark=triangle,
				]
				coordinates {
					(0.5,100)(1.0,100)(1.2,98)(1.5,90)(2.0,85)
				};
				\addlegendentry{Low Light}
				
				\addplot[
				color=green,
				mark=o,
				]
				coordinates {
					(0.5,100)(1.0,100)(1.5,100)(1.7,99)(2.0,93)
				};
				\addlegendentry{Bright Light}
			\end{axis}
		\end{tikzpicture}
		\caption{ArUco Marker Detection Rate vs. Distance}
		\label{fig:detection_rate}
	\end{minipage}%
	\hspace{0.03\textwidth}  % Add explicit horizontal spacing between figures
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\begin{tikzpicture}[scale=1]  % Slightly scaled down to ensure margin compliance
			\begin{axis}[
				ybar,
				bar width=0.5cm,
				enlarge x limits=0.15,
				ylabel={Detection Rate (\%)},
				xlabel={Angle (°)},
				symbolic x coords={0, 15, 30, 45, 60},
				xtick=data,
				ymin=0, ymax=110,
				nodes near coords,
				grid=major,
				ymajorgrids=true,
				grid style=dashed,
				width=\textwidth  % Ensure figure fits within minipage
				]
				\addplot coordinates {(0,100) (15,100) (30,100) (45,95) (60,85)};
			\end{axis}
		\end{tikzpicture}
		\caption{Detection Rate vs. Angle for ArUco Marker Detection}
		\label{fig:angle_detection}
	\end{minipage}
\end{figure}

\textbf{Angle Robustness:} 
For angular testing, 100 images were captured at angles from 0° to 60° in 15° increments. Detection rates were analyzed to understand performance under different orientations.



\subsubsection{Distance Measurement Testing}
\textbf{Data Collection:} 
Measurements were taken using the \texttt{estimate\_pose} function at fixed distances (0.5m, 1m, 1.5m, 2m), with 100 samples at each point.

\textbf{Analysis:} 
The Mean Absolute Error (MAE) was calculated between measured and actual distances. Consistency was tested by taking 500 consecutive measurements at 1m, calculating standard deviations and confidence intervals.

\begin{table}[H]
	\centering
	\caption{Distance Measurement Accuracy}
	\label{tab:distance_accuracy}
	\begin{tabular}{ccc}
		\hline
		\textbf{Actual Distance} & \textbf{Mean Absolute Error} & \textbf{Error Percentage} \\
		\hline
		0.5m & 0.015m & 3.0\% \\
		1.0m & 0.025m & 2.5\% \\
		1.5m & 0.045m & 3.0\% \\
		2.0m & 0.070m & 3.5\% \\
		\hline
	\end{tabular}
\end{table}


These comprehensive tests validate the ArUco marker detection system's performance, accuracy, and reliability across various conditions. The results demonstrate the system's capability to provide accurate distance measurements and speed estimations, crucial for the robot's navigation and interaction with its environment. The high detection rates and low processing times confirm the suitability of ArUco markers for our real-time robotics application, aligning with the findings of Patru et al. \cite{patru2023empirical}.

The integration testing results confirm that the complete system can operate in real-time, successfully detecting markers, estimating distances and speeds, and responding to control zones. These findings provide a solid foundation for the robot's ability to navigate and interact with its environment using ArUco markers as reference points.





\section{Subsystem Combinations Testing}

After independent testing, the next phase was to combine the subsystems and test their interactions.

\subsection{Camera and Web Interface Integration}

The integration of the camera feed into the web interface was essential for providing real-time visual feedback to the user. The camera was initially connected to the Raspberry Pi, and a Flask-based web interface was used to stream the video feed. The initial implementation focused on setting up the basic functionality of streaming the live video feed through a web page.

\subsubsection{Testing Video Streaming Performance}

Once the camera feed was integrated into the web interface, the system was tested to evaluate its performance. During the initial tests, it was observed that the video stream had a significant lag of \textbf{2-3 seconds}. This was primarily due to the high frame rate of \textbf{30 FPS} (frames per second), which caused data transmission to overwhelm the available bandwidth. The delay resulted in a poor user experience, especially when real-time responsiveness was crucial for controlling the robot.

To assess the performance:
\begin{itemize}
	\item The latency was measured as the delay between the video captured by the camera and its display on the web interface.
	\item The frame rate, resolution, and bandwidth usage were analyzed to determine the cause of the performance bottlenecks.
	\item It was also noted that the web browser’s buffering behavior contributed to the observed lag.
\end{itemize}

\subsubsection{Performance Optimization and Improvements}

To improve the streaming performance, several optimizations were implemented. The following changes were made:

\begin{itemize}
	\item \textbf{Frame Rate Reduction}: The frame rate was reduced from \textbf{60 FPS} to \textbf{30 FPS}. This lowered the amount of data transmitted per second, reducing the load on both the network and the Raspberry Pi, thereby decreasing the lag.
	\item \textbf{Resolution Adjustment}: The resolution of the video feed was scaled down to \textbf{640x480}. This significantly reduced the bandwidth consumption, enabling a smoother video feed without affecting the user's ability to interact with the system effectively.
\end{itemize}

\textbf{Final Results:}
\begin{itemize}
	\item Initial lag: \textbf{2-3 seconds} at 60 FPS.
	\item Optimized lag: \textbf{500ms} at 30 FPS with \textbf{320x240} resolution.
	\item Smooth, reliable video streaming with minimal frame drops.
\end{itemize}

This phase of testing ensured that the camera feed was effectively integrated into the web interface and optimized to provide responsive, real-time feedback necessary for controlling the robot.


\subsection{Web Interface and Motor Control Integration}

For integrating the web interface and motor control, the following steps were taken:

\begin{enumerate}
	\item Developing a separate program to listen for key inputs (W, A, S, D).
	\item Using SocketIO for real-time communication between the web interface and motor control program.
	\item Creating an interface to send PWM signals from the web interface to the motors via the H-bridge.
\end{enumerate}


% Now, include the code:
\begin{lstlisting}[style=pythonstyle, caption=SocketIO event handler for robot movement]
	# SocketIO event to handle robot movement
	@socketio.on('move')
	def handle_move(direction):
		global forward_disabled
		if direction == 'forward' and forward_disabled:
			print("Forward movement is disabled [restricted zone]")
			robot.stop()
		elif direction == 'forward':
			robot.move_forward()
		elif direction == 'backward':
			robot.move_backward()
		elif direction == 'left':
			robot.turn_left()
		elif direction == 'right':
			robot.turn_right()
		elif direction == 'stop':
			robot.stop()
\end{lstlisting}



The web interface successfully controlled the robot's movements, with a measured control latency of \textbf{300ms}. Minor delays were noted during rapid direction changes, which could require further optimization.

\textbf{Results:}
\begin{itemize}
	\item Control latency: \textbf{300ms}.
	\item Issues: Slight delay during rapid changes.
\end{itemize}

\subsection{Web Interface and ArUco Detection Integration}

To integrate ArUco marker detection into the web interface, the camera feed was processed in real time, and marker IDs were displayed on the interface. The system could detect markers with a delay of \textbf{100ms} when more than three markers were present in the frame simultaneously.

\textbf{Results:}
\begin{itemize}
	\item Detection delay: \textbf{100ms} with three or more markers.
	\item Detection accuracy: \textbf{100\%} for up to three markers.
\end{itemize}

\section{Integration Phase}

The final integration phase brought together the core components: \textbf{Aruco marker detection}, \textbf{motor control}, and the \textbf{web interface}. This stage focused on combining the previously tested individual systems into a unified framework capable of real-time robot control and marker detection.

Key steps included:
\begin{enumerate}
	\item Merging the motor control program with the ArUco marker detection system to allow the robot to respond to detected markers.
	\item Real-time processing of the video feed from the camera for marker detection, alongside real-time control through the web interface.
	\item Implementing pre-defined behaviors that adjusted motor actions (such as speed and direction changes) based on the specific markers detected by the camera.
\end{enumerate}

Features such as distance measurement, speed estimation, and marker-based tasks were also developed and tested independently before being added to the full system. These functionalities were integrated step-by-step to ensure proper interaction between components.

A more detailed discussion of the final system’s performance, including its compliance with the Acceptance Test Procedures (ATPs), will be covered in the next chapter. There, the results of the fully integrated system will be examined in line with the project's objectives.